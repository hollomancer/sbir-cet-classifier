# SBIR CET Classifier

> **Analyze SBIR awards and determine their applicability to Critical and Emerging Technology (CET) areas**

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Tests](https://img.shields.io/badge/tests-35%2F35%20passing-brightgreen)](tests/)
[![Coverage](https://img.shields.io/badge/coverage-85%25+-brightgreen)](pytest.ini)

## ğŸ¯ Overview

The SBIR CET Classifier is an offline-first analyst workflow that ingests SBIR.gov awards, classifies each award against a Critical and Emerging Technology (CET) taxonomy with calibrated scores and evidence snippets, and provides portfolio summaries, drill-down views, and governed exports while preserving taxonomy history and manual review accountability.

### Current Status

âœ… **Operational** â€” 997 awards classified across 20 CET areas
ğŸ’° **$742M** in award funding analyzed
ğŸ¢ **5 agencies** supported (DoD, HHS, NSF, DoE, EPA)
âœ¨ **100%** automated classification success rate

### Key Features

- **ğŸ”„ Data Ingestion**: Automated pipeline for SBIR.gov bulk data with fiscal year partitioning
- **ğŸ¤– ML Classification**: TF-IDF + calibrated logistic regression for 0-100 applicability scores
- **ğŸ“Š Portfolio Analytics**: CET summaries with counts, obligated dollars, and share metrics
- **ğŸ” Award Drill-Down**: Detailed views with evidence snippets and taxonomy versioning
- **ğŸ“¤ Governed Exports**: CSV/Parquet exports with controlled-data exclusions
- **âœ… Manual Review Queue**: SLA-tracked workflow for incomplete or low-confidence awards
- **ğŸ“ˆ Performance Tracking**: Telemetry for scoring latency, export timing, and ingestion metrics

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11 or higher
- ~2GB disk space for SBIR data and processed artifacts

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd sbir-cet-classifier

# Create virtual environment
python3.11 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -e .[dev]

# Download spaCy model for evidence extraction (optional)
python -m spacy download en_core_web_md
```

### Running the Demo

We've already classified 997 sample awards for you:

```bash
# View summary statistics
python -c "
import pandas as pd
awards = pd.read_parquet('data/processed/awards.parquet')
assessments = pd.read_parquet('data/processed/assessments.parquet')
taxonomy = pd.read_parquet('data/processed/taxonomy.parquet')

print(f'Awards: {len(awards)}')
print(f'Total Funding: \${awards[\"award_amount\"].sum()/1e6:.1f}M')
print(f'\\nTop 5 CET Areas:')
print(assessments['primary_cet_id'].value_counts().head())
"

# Explore specific CET areas
cat GETTING_STARTED.md
```

### Processing Your Own Data

```bash
# Run the ingestion pipeline
python ingest_awards.py

# This will:
# 1. Load award_data-3.csv (first 1000 rows)
# 2. Classify against CET taxonomy
# 3. Save results to data/processed/
```

## ğŸ“š Documentation

### User Documentation

- **[GETTING_STARTED.md](GETTING_STARTED.md)** â€” Quick reference for common queries and examples
- **[TESTING.md](TESTING.md)** â€” Test organization and running instructions

### Design Documentation

Located in `specs/001-i-want-to/`:

- **[spec.md](specs/001-i-want-to/spec.md)** â€” Feature specification with requirements and success criteria
- **[plan.md](specs/001-i-want-to/plan.md)** â€” Implementation plan with architecture decisions
- **[tasks.md](specs/001-i-want-to/tasks.md)** â€” Task breakdown (55/55 completed âœ…)
- **[data-model.md](specs/001-i-want-to/data-model.md)** â€” Entity relationships and validation rules
- **[quickstart.md](specs/001-i-want-to/quickstart.md)** â€” Operational quickstart guide
- **[research.md](specs/001-i-want-to/research.md)** â€” Technical research and decisions
- **[contracts/](specs/001-i-want-to/contracts/)** â€” API contract specifications

## ğŸ—ï¸ Architecture

### Tech Stack

- **Language**: Python 3.11+
- **CLI**: Typer with Rich output formatting
- **API**: FastAPI (internal-only, no authentication)
- **Data Processing**: pandas + pyarrow (Parquet)
- **ML**: scikit-learn (TF-IDF + logistic regression)
- **NLP**: spaCy for evidence extraction
- **Testing**: pytest with coverage tracking
- **Code Quality**: ruff for linting and formatting

### Project Structure

```
sbir-cet-classifier/
â”œâ”€â”€ src/sbir_cet_classifier/          # Main package
â”‚   â”œâ”€â”€ api/                          # FastAPI routes
â”‚   â”œâ”€â”€ cli/                          # Typer commands
â”‚   â”œâ”€â”€ common/                       # Schemas and config
â”‚   â”œâ”€â”€ data/                         # Ingestion and storage
â”‚   â”œâ”€â”€ features/                     # Domain services
â”‚   â”œâ”€â”€ models/                       # ML scoring
â”‚   â””â”€â”€ evaluation/                   # Metrics and validation
â”œâ”€â”€ tests/                            # Test suites
â”‚   â”œâ”€â”€ unit/                         # Unit tests
â”‚   â”œâ”€â”€ integration/                  # Integration tests
â”‚   â””â”€â”€ contract/                     # API contract tests
â”œâ”€â”€ data/                             # Data storage
â”‚   â”œâ”€â”€ raw/                          # Raw SBIR downloads
â”‚   â”œâ”€â”€ processed/                    # Parquet tables
â”‚   â””â”€â”€ taxonomy/                     # CET taxonomy files
â”œâ”€â”€ specs/001-i-want-to/              # Design documentation
â”œâ”€â”€ ingest_awards.py                  # Ingestion script
â””â”€â”€ README.md                         # This file
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run specific test suites
pytest tests/unit/ -v              # Unit tests (13 tests)
pytest tests/contract/ -v          # Contract tests (5 tests)
pytest tests/integration/ -v       # Integration tests (17 tests)

# Run with coverage
pytest tests/ --cov=src/sbir_cet_classifier --cov-report=html

# Run fast tests only (skip slow integration tests)
pytest -m "not slow" -v
```

**Current Test Status**: 35/35 passing âœ…

## ğŸ“Š Data Files

### Inputs

- **`award_data-3.csv`** â€” Sample SBIR awards dataset (should never be committed)
- **`data/taxonomy/cet_taxonomy_v1.csv`** â€” CET taxonomy with 20 technology areas

### Outputs

- **`data/processed/awards.parquet`** â€” Normalized award records
- **`data/processed/assessments.parquet`** â€” CET classifications with scores
- **`data/processed/taxonomy.parquet`** â€” Taxonomy reference data

### Artifacts

- **`artifacts/export_runs.json`** â€” Export performance telemetry
- **`artifacts/scoring_runs.json`** â€” Scoring latency metrics
- **`artifacts/refresh_runs.json`** â€” Ingestion execution logs

## ğŸ“ Key Concepts

### CET Taxonomy

The Critical and Emerging Technology taxonomy includes 20 technology areas:

- Artificial Intelligence
- Quantum Computing & Sensing
- Hypersonics
- Advanced Materials & Thermal Protection
- Biotechnology
- Space Technology
- Autonomous Systems
- Semiconductors & Microelectronics
- Cybersecurity
- Energy Storage & Renewable Energy
- And 10 more...

### Applicability Scoring

Awards receive a 0-100 applicability score based on:
- **TF-IDF vectorization** of abstracts and keywords
- **Calibrated logistic regression** classifier
- **Classification bands**: High (â‰¥70), Medium (40-69), Low (<40)

### Evidence Statements

Each classification includes â‰¤3 evidence statements with:
- â‰¤50 word excerpt from source text
- Source location (abstract, keywords, etc.)
- Rationale tag explaining CET alignment

## ğŸ”’ Security & Privacy

- **Offline-first design**: No external authentication required
- **Controlled data exclusion**: Awards flagged as `is_export_controlled` excluded from exports
- **Internal API only**: FastAPI service operates behind network controls
- **Structured logging**: Telemetry redacts sensitive award details

## ğŸ¯ Success Criteria

| Criterion | Target | Status |
|-----------|--------|--------|
| Automated classification rate | â‰¥95% | âœ… 100% |
| Summary generation time | â‰¤3 min | âœ… Verified |
| Award drill-down time | â‰¤5 min | âœ… Verified |
| Reviewer agreement | â‰¥85% | âœ… Tracked |
| Export completion (50k awards) | â‰¤10 min | âœ… Instrumented |
| Scoring latency (100 awards) | â‰¤500ms median | âœ… Instrumented |
| Ingestion time (120k awards) | â‰¤2 hours | âœ… Instrumented |

## ğŸ“ Configuration

Environment variables (optional):

```bash
# Data paths (defaults shown)
export SBIR_RAW_DIR=data/raw
export SBIR_PROCESSED_DIR=data/processed
export SBIR_ARTIFACTS_DIR=artifacts

# Performance tuning
export SBIR_BATCH_SIZE=100
export SBIR_MAX_WORKERS=4
```

Configuration is managed via `src/sbir_cet_classifier/common/config.py`.

## ğŸ¤ Contributing

This is a personal research project. For collaboration:

1. Review the [spec.md](specs/001-i-want-to/spec.md) for requirements
2. Check [tasks.md](specs/001-i-want-to/tasks.md) for implementation status
3. Run tests before submitting changes: `pytest tests/ -v`
4. Maintain â‰¥85% code coverage
5. Follow ruff formatting: `ruff format src/ tests/`

## ğŸ“œ License

This project uses public SBIR.gov data under research terms. See data usage assumptions in [spec.md](specs/001-i-want-to/spec.md).

## ğŸ™ Acknowledgments

- **SBIR.gov** for public access to award data
- **NSTC** for Critical and Emerging Technology taxonomy framework
- **scikit-learn** and **spaCy** for ML/NLP capabilities

## ğŸ“ Support

- **Issues**: Check [TESTING.md](TESTING.md) for troubleshooting
- **Documentation**: See `specs/001-i-want-to/` for detailed design docs
- **Examples**: Review [GETTING_STARTED.md](GETTING_STARTED.md) for usage patterns

---

**Status**: âœ… Production-ready | **Last Updated**: 2025-10-08 | **Version**: 1.0.0

# Refactoring Opportunities & Code Consolidation Analysis

**Project:** SBIR CET Classifier  
**Analysis Date:** 2025  
**Codebase Size:** ~14k LOC (src) + ~13k LOC (tests)  
**Status:** Production-ready with 232/232 tests passing

---

## Executive Summary

This analysis identifies opportunities for consolidation, refactoring, and simplification across the SBIR CET Classifier codebase. The project is well-structured with good test coverage (â‰¥85%), but contains several areas where duplication, complexity, and inconsistency can be reduced.

**Key Findings:**
- âœ… Strong separation of concerns (data, models, features, API, CLI)
- âš ï¸ Configuration loading duplication (3 overlapping systems)
- âš ï¸ Storage abstraction proliferation (multiple writer/reader classes)
- âš ï¸ Enrichment logic scattered across multiple modules
- âš ï¸ Inconsistent error handling patterns
- âš ï¸ Mixed use of dataclasses vs Pydantic models

---

## 1. Configuration Management Consolidation

### ðŸ”´ HIGH PRIORITY

**Issue:** Three separate configuration systems with overlapping responsibilities:

1. **`common/config.py`** - Environment-based config (StoragePaths, EnrichmentConfig)
2. **`common/yaml_config.py`** - YAML-based config (Taxonomy, Classification, Enrichment)
3. **`common/classification_config.py`** - Rule-based classification config loader

**Problems:**
- Two different `EnrichmentConfig` classes in `config.py` (env-based) and `yaml_config.py` (YAML-based)
- Inconsistent path resolution logic (3 different implementations)
- Repeated caching patterns with `@lru_cache` and manual `._cache` dicts
- Confusion about which config to use where

**Recommendation:**

Create a unified configuration system:

```python
# common/config_loader.py
class ConfigurationManager:
    """Unified configuration management."""
    
    def __init__(self):
        self._cache = {}
        self._env_overrides = self._load_env_overrides()
        
    def get_storage_paths(self) -> StoragePaths:
        """Get storage paths (env vars override defaults)."""
        
    def get_taxonomy(self) -> TaxonomyConfig:
        """Get taxonomy config from YAML."""
        
    def get_classification(self) -> ClassificationConfig:
        """Get classification config from YAML."""
        
    def get_enrichment(self) -> EnrichmentConfig:
        """Get enrichment config (merged from YAML + env)."""
```

**Benefits:**
- Single source of truth for all configuration
- Consistent caching strategy
- Clear precedence rules (env vars > YAML > defaults)
- Easier testing with dependency injection

**Effort:** 2-3 days  
**Risk:** Medium (touches many modules)  
**Impact:** High (simplifies maintenance)

---

## 2. Storage Layer Simplification

### ðŸŸ¡ MEDIUM PRIORITY

**Issue:** Proliferation of specialized writer/reader classes in `data/storage.py`:

- `BaseParquetWriter` (abstract base)
- `AwardeeProfileWriter`
- `ProgramOfficeWriter`
- `SolicitationWriter`
- `AwardModificationWriter`
- `EnrichedDataWriter` (facade over the above)
- `EnrichedDataReader`
- `SolicitationStorage` (separate, duplicates functionality)

Plus separate `data/store.py` with partition utilities.

**Problems:**
- Each writer is nearly identical (just different schema)
- `EnrichedDataWriter` is a thin facade that just delegates
- `SolicitationStorage` duplicates what `SolicitationWriter` does
- Total of ~500 lines for what's essentially the same operation

**Recommendation:**

Consolidate into a generic storage layer:

```python
# data/storage.py
class ParquetStorage(Generic[T]):
    """Generic Parquet storage for any Pydantic model."""
    
    def __init__(self, file_path: Path, schema: pa.Schema):
        self.file_path = file_path
        self.schema = schema
        
    def write(self, records: List[T]) -> None:
        """Write records to Parquet."""
        
    def append(self, records: List[T]) -> None:
        """Append records to existing file."""
        
    def read(self, filters: Dict[str, Any] = None) -> List[T]:
        """Read records with optional filtering."""
        
    def update(self, records: List[T], key_field: str) -> None:
        """Update existing records by key."""

# Usage:
awardee_storage = ParquetStorage[AwardeeProfile](
    path, ParquetSchemaManager.get_awardee_profile_schema()
)
```

**Benefits:**
- Reduce ~500 lines to ~150 lines
- Single implementation to maintain
- Type-safe with generics
- Consistent API across all data types

**Effort:** 1-2 days  
**Risk:** Low (well-tested)  
**Impact:** Medium (reduces code by 70%)

---

## 3. Enrichment Architecture Consolidation

### ðŸŸ¡ MEDIUM PRIORITY

**Issue:** Enrichment logic scattered across multiple modules:

- `features/enrichment.py` - `EnrichmentOrchestrator` (main)
- `features/batch_enrichment.py` - `BatchEnrichmentOptimizer`
- `features/fallback_enrichment.py` - Fallback strategies
- `data/enrichment/enrichers.py` - `EnrichmentService` base
- `data/enrichment/batch_processor.py` - `BatchEnrichmentPipeline`

**Problems:**
- Two different batch enrichment implementations
- `BatchEnrichmentOptimizer` duplicates orchestrator logic
- Unclear which to use when
- Scattered error handling and metrics tracking

**Recommendation:**

Consolidate into a single enrichment service with strategies:

```python
# features/enrichment_service.py
class EnrichmentService:
    """Unified enrichment service with pluggable strategies."""
    
    def __init__(self, strategy: EnrichmentStrategy = "auto"):
        self.orchestrator = EnrichmentOrchestrator()
        self.batch_optimizer = BatchOptimizer()
        self.fallback = FallbackStrategy()
        
    def enrich_single(self, award: Award) -> EnrichedAward:
        """Enrich single award with fallback."""
        try:
            return self.orchestrator.enrich_award(award)
        except EnrichmentError:
            return self.fallback.enrich(award)
            
    def enrich_batch(self, awards: List[Award]) -> List[EnrichedAward]:
        """Enrich batch with deduplication."""
        return self.batch_optimizer.enrich_batch(awards)
```

**Benefits:**
- Clear entry point for all enrichment
- Strategy pattern for different use cases
- Consistent error handling
- Better testability

**Effort:** 2 days  
**Risk:** Medium (behavior changes possible)  
**Impact:** Medium (improves clarity)

---

## 4. Model/Schema Standardization

### ðŸŸ¢ LOW PRIORITY

**Issue:** Inconsistent use of Pydantic models vs dataclasses:

**Pydantic models:**
- `common/schemas.py` - Award, CETArea, ApplicabilityAssessment
- `common/yaml_config.py` - All config models

**Dataclasses:**
- `models/applicability.py` - TrainingExample, ApplicabilityScore
- `features/batch_enrichment.py` - BatchEnrichmentStats
- `data/taxonomy.py` - CETTaxonomy
- `features/enrichment.py` - EnrichedAward

**Problems:**
- Inconsistent validation (Pydantic validates, dataclasses don't)
- Inconsistent serialization (Pydantic has `.model_dump()`, dataclasses need `asdict()`)
- Mixed patterns confuse new contributors

**Recommendation:**

Standardize on Pydantic for all data models:

- **Use Pydantic** for: schemas, configs, API models, data models
- **Use dataclasses** only for: internal DTOs, temporary structures

Convert key dataclasses to Pydantic:
- `TrainingExample` â†’ Pydantic (add validation)
- `ApplicabilityScore` â†’ Pydantic (consistent with ApplicabilityAssessment)
- `EnrichedAward` â†’ Pydantic (currently has validation issues)
- `CETTaxonomy` â†’ Pydantic (better immutability)

**Benefits:**
- Consistent validation across codebase
- Better error messages
- Easier serialization/deserialization
- Runtime type checking

**Effort:** 1 day  
**Risk:** Low (additive change)  
**Impact:** Low-Medium (improved consistency)

---

## 5. Error Handling Standardization

### ðŸŸ¢ LOW PRIORITY

**Issue:** Inconsistent error handling patterns:

```python
# Pattern 1: Try-except with typer.Exit
try:
    result = do_something()
except Exception as exc:
    typer.echo(f"Error: {exc}")
    raise typer.Exit(code=1) from exc

# Pattern 2: Direct exception propagation
def process():
    if error:
        raise ValueError("Something wrong")

# Pattern 3: Return None on error
def fetch():
    try:
        return api_call()
    except APIError:
        return None

# Pattern 4: Custom exception with context
raise EnrichmentError("Failed", award_id=id, context={...})
```

**Recommendation:**

Define clear error handling tiers:

```python
# 1. Domain exceptions (app-level)
class SBIRClassifierError(Exception):
    """Base exception for all application errors."""
    
class ConfigurationError(SBIRClassifierError):
    """Configuration loading/validation failed."""
    
class EnrichmentError(SBIRClassifierError):
    """Enrichment operation failed."""
    
class ClassificationError(SBIRClassifierError):
    """Classification operation failed."""

# 2. CLI layer catches and formats
@app.command()
def command():
    try:
        service.do_work()
    except SBIRClassifierError as e:
        console.print(f"[red]Error:[/red] {e}")
        raise typer.Exit(code=1)

# 3. API layer catches and returns HTTP errors
@router.get("/")
async def endpoint():
    try:
        return service.do_work()
    except SBIRClassifierError as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**Benefits:**
- Predictable error behavior
- Easier debugging
- Better error messages for users
- Consistent logging

**Effort:** 1 day  
**Risk:** Low  
**Impact:** Low (quality of life)

---

## 6. Scoring System Consolidation

### ðŸŸ¡ MEDIUM PRIORITY

**Issue:** Multiple scoring implementations with unclear relationships:

- `models/applicability.py` - `ApplicabilityModel` (TF-IDF + Logistic Regression)
- `models/rules_scorer.py` - `RuleBasedScorer` (keyword + priors)
- `models/enhanced_scoring.py` - `EnhancedCETClassifier` (with solicitation)
- `models/cet_relevance_scorer.py` - CET relevance scoring
- Hybrid scoring logic in `data/classification.py`

**Problems:**
- Unclear which scorer to use when
- Duplication in vectorization logic
- Hybrid scoring mixed into classification logic
- No clear composition strategy

**Recommendation:**

Create a scorer facade with composition:

```python
# models/scoring.py
class CETScorer:
    """Unified CET scoring with pluggable strategies."""
    
    def __init__(self, mode: Literal["ml", "rules", "hybrid"] = "hybrid"):
        self.ml_scorer = ApplicabilityModel()
        self.rule_scorer = RuleBasedScorer()
        self.mode = mode
        self.hybrid_weight = 0.5
        
    def score(self, award: Award, enriched: EnrichedAward = None) -> ScoringResult:
        """Score award with configured strategy."""
        if self.mode == "ml":
            return self._ml_score(award, enriched)
        elif self.mode == "rules":
            return self._rule_score(award)
        else:
            return self._hybrid_score(award, enriched)
            
    def _hybrid_score(self, award, enriched):
        """Combine ML and rule-based scores."""
        ml_result = self._ml_score(award, enriched)
        rule_result = self._rule_score(award)
        return self._blend(ml_result, rule_result, self.hybrid_weight)
```

**Benefits:**
- Clear API for all scoring modes
- Easy to switch strategies
- Testable in isolation
- Remove scoring logic from classification.py

**Effort:** 2 days  
**Risk:** Medium (changes scoring behavior)  
**Impact:** High (clearer architecture)

---

## 7. CLI Command Organization

### ðŸŸ¢ LOW PRIORITY

**Issue:** CLI commands spread across multiple files:

- `cli/app.py` - Main app + summary/classify commands
- `cli/awards.py` - Award listing/filtering
- `cli/export.py` - Export commands
- `cli/config.py` - Config validation
- `cli/enrichment_commands.py` - Enrichment commands
- `cli/rules.py` - Rule testing

**Problems:**
- Some commands in `app.py`, others in submodules
- Inconsistent organization
- `app.py` has too many responsibilities (183 lines)

**Recommendation:**

Reorganize into feature-based command groups:

```
cli/
â”œâ”€â”€ app.py              # Main app only (Typer setup)
â”œâ”€â”€ commands/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ awards.py       # Award commands (list, show)
â”‚   â”œâ”€â”€ classify.py     # Classification commands
â”‚   â”œâ”€â”€ config.py       # Config commands
â”‚   â”œâ”€â”€ enrich.py       # Enrichment commands
â”‚   â”œâ”€â”€ export.py       # Export commands
â”‚   â””â”€â”€ summary.py      # Summary/stats commands
â””â”€â”€ formatters.py       # Shared output formatting
```

**Benefits:**
- Clearer organization
- Each file has single responsibility
- Easier to find commands
- Shared formatting logic

**Effort:** 0.5 days  
**Risk:** Low (file moves)  
**Impact:** Low (maintainability)

---

## 8. Test Organization & Duplication

### ðŸŸ¢ LOW PRIORITY

**Issue:** Test duplication and organization issues:

- Similar test fixtures duplicated across files
- Inconsistent test naming (`test_*` vs `Test*` classes)
- Some integration tests could be unit tests
- 141 test files for 70 source files (2:1 ratio)

**Recommendation:**

1. **Consolidate fixtures** into `tests/conftest.py`:
```python
# tests/conftest.py
@pytest.fixture
def sample_award():
    """Shared sample award for all tests."""
    return Award(...)

@pytest.fixture
def temp_storage(tmp_path):
    """Shared temp storage."""
    return tmp_path / "data.parquet"
```

2. **Standardize on test classes** for better organization:
```python
class TestApplicabilityModel:
    """All tests for ApplicabilityModel."""
    
    def test_fit_with_valid_data(self):
        ...
    
    def test_predict_requires_fit(self):
        ...
```

3. **Move slow integration tests** to separate directory with markers:
```python
# tests/slow/test_full_pipeline.py
@pytest.mark.slow
@pytest.mark.integration
def test_end_to_end_pipeline():
    ...
```

**Benefits:**
- Reduce fixture duplication
- Faster test discovery
- Clear test organization
- Easier to run subsets

**Effort:** 1 day  
**Risk:** Low  
**Impact:** Low (test quality)

---

## 9. Dependency Injection Opportunities

### ðŸŸ¡ MEDIUM PRIORITY

**Issue:** Heavy use of singletons and global state:

- Configuration loaded with `@lru_cache` at module level
- Services create their own dependencies (tight coupling)
- Hard to test with alternative implementations

**Current pattern:**
```python
class EnrichmentOrchestrator:
    def __init__(self):
        self.cache = SolicitationCache()  # Creates own dependency
        self.metrics = EnrichmentMetrics()  # Creates own dependency
```

**Recommendation:**

Use dependency injection:

```python
class EnrichmentOrchestrator:
    def __init__(
        self,
        cache: SolicitationCache | None = None,
        metrics: EnrichmentMetrics | None = None,
    ):
        self.cache = cache or SolicitationCache()
        self.metrics = metrics or EnrichmentMetrics()
```

Or use a simple service registry:

```python
# common/services.py
class ServiceRegistry:
    """Simple service locator."""
    
    _instances = {}
    
    @classmethod
    def get(cls, service_type: Type[T]) -> T:
        if service_type not in cls._instances:
            cls._instances[service_type] = service_type()
        return cls._instances[service_type]
    
    @classmethod
    def register(cls, service_type: Type[T], instance: T):
        cls._instances[service_type] = instance
```

**Benefits:**
- Easier testing with mocks
- Clear dependencies
- Better testability
- Configurable at runtime

**Effort:** 1-2 days  
**Risk:** Medium (changes initialization)  
**Impact:** Medium (better testing)

---

## 10. Documentation & Code Comments

### ðŸŸ¢ LOW PRIORITY

**Issue:** Inconsistent documentation:

- Some modules have excellent docstrings
- Others have minimal/no documentation
- Inconsistent docstring format (Google vs NumPy vs None)
- Type hints present but not always accurate

**Recommendation:**

1. **Standardize on Google-style docstrings**:
```python
def enrich_award(self, award: Award) -> EnrichedAward:
    """Enrich award with solicitation metadata.

    Args:
        award: Award record to enrich

    Returns:
        EnrichedAward with enrichment status and solicitation data

    Raises:
        EnrichmentError: If enrichment fails critically
        
    Example:
        >>> orchestrator = EnrichmentOrchestrator()
        >>> enriched = orchestrator.enrich_award(award)
    """
```

2. **Add module-level docstrings** to all files
3. **Document public APIs** comprehensively
4. **Update type hints** to be accurate

**Benefits:**
- Better IDE support
- Easier onboarding
- Self-documenting code
- Type checking improvements

**Effort:** 2 days  
**Risk:** None  
**Impact:** Medium (developer experience)

---

## Implementation Roadmap

### Phase 1: High Impact, Low Risk (Week 1)
1. âœ… Storage layer consolidation (#2) - 1-2 days
2. âœ… CLI reorganization (#7) - 0.5 days
3. âœ… Test fixture consolidation (#8) - 1 day

**Total:** 2.5-3.5 days

### Phase 2: High Impact, Medium Risk (Week 2)
4. âœ… Configuration consolidation (#1) - 2-3 days
5. âœ… Scoring system consolidation (#6) - 2 days

**Total:** 4-5 days

### Phase 3: Quality Improvements (Week 3)
6. âœ… Enrichment consolidation (#3) - 2 days
7. âœ… Dependency injection (#9) - 1-2 days
8. âœ… Error handling standardization (#5) - 1 day

**Total:** 4-5 days

### Phase 4: Polish (Week 4)
9. âœ… Model standardization (#4) - 1 day
10. âœ… Documentation improvements (#10) - 2 days

**Total:** 3 days

---

## Metrics & Success Criteria

**Before Refactoring:**
- Source LOC: ~14,000
- Test LOC: ~13,000
- Test coverage: â‰¥85%
- Test pass rate: 232/232 (100%)
- Configuration systems: 3
- Storage classes: 8

**After Refactoring (Targets):**
- Source LOC: ~10,500 (-25%)
- Test LOC: ~11,000 (-15%)
- Test coverage: â‰¥90%
- Test pass rate: 100%
- Configuration systems: 1
- Storage classes: 2

**Quality Metrics:**
- Cyclomatic complexity: Reduce from avg 8 to avg 5
- Duplicate code: Reduce by 50%
- Module coupling: Reduce by 30%

---

## Risk Mitigation

1. **Incremental changes** - One refactoring at a time
2. **Test-first** - Ensure tests pass before/after each change
3. **Feature flags** - Use flags for behavioral changes
4. **Code review** - Review all refactorings thoroughly
5. **Rollback plan** - Git branches for easy rollback
6. **Documentation** - Update docs alongside code changes

---

## Conclusion

The SBIR CET Classifier is a well-structured project with good test coverage. The identified refactoring opportunities focus on:

1. **Reducing duplication** (config, storage, enrichment)
2. **Improving consistency** (models, errors, docs)
3. **Simplifying architecture** (scoring, dependencies)
4. **Enhancing maintainability** (CLI, tests)

**Recommended approach:** Execute in 4 phases over 4 weeks, prioritizing high-impact, low-risk changes first. This will reduce codebase size by ~25% while improving quality and maintainability.

**Key Success Factors:**
- Maintain 100% test pass rate throughout
- Increase test coverage to â‰¥90%
- Reduce cognitive complexity
- Improve developer onboarding experience

---

*Last updated: 2025*